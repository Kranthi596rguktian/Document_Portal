{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5602aa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all okay\n"
     ]
    }
   ],
   "source": [
    "print(\"all okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5140be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52654b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e97b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model='qwen/qwen3-32b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4336e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <think>\n",
      "Okay, so the user is asking, \"What is the capital of France?\" Let me think about how to approach this.\n",
      "\n",
      "First, I know that capitals are usually well-known, but maybe there's a chance someone could confuse it with another city. France is a country in Europe, right? So, the capital city I remember is Paris. But wait, let me double-check to make sure there's not a trick here. Sometimes people ask about countries with similar names or regions that might have different capitals.\n",
      "\n",
      "Wait, is there a country named \"France\" that isn't in Europe? No, France is a country in Western Europe. The capital city is Paris. I'm pretty confident about that. Let me think if there's any other possible answer. Maybe Lyon or Marseille? No, those are major cities in France, but they are not the capital. The capital is definitely Paris. \n",
      "\n",
      "Also, considering other countries with similar names, like French Guiana or French Polynesia, but those are territories, not the main country. The question specifically says \"France,\" so the answer is Paris. \n",
      "\n",
      "Just to be thorough, maybe the user is a student or someone learning geography. So providing a clear and accurate answer is key. No need for extra details unless the user asks for more. But since the question is straightforward, a direct answer with a brief confirmation should suffice. Yeah, I think that's it. Paris is the capital of France.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**. It is renowned for its historical significance, cultural landmarks, and as a global hub for art, fashion, and cuisine. Let me know if you'd like additional details! ðŸ‡«ðŸ‡·\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "response = model.invoke(query).content\n",
    "\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb0fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyBW1GC8N3pJRUWplpY_Hs8XjqC915d_D5U'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ccdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb2c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model='models/gemini-embedding-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0dd4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd = embedding_model.embed_query(\"What is the capital of France?\")\n",
    "len(emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82215972",
   "metadata": {},
   "source": [
    "### 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49746814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ae83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4446f050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Kranthi\\\\OneDrive\\\\Desktop\\\\LLMOPS_Krishnaik\\\\Projects\\\\Document_Portal\\\\notebook'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e5eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), 'data', 'sample.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc8e6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cdc1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2333dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7792e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b39d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20530dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'c:\\\\Users\\\\Kranthi\\\\OneDrive\\\\Desktop\\\\LLMOPS_Krishnaik\\\\Projects\\\\Document_Portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a02fcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvronâˆ— Louis Martinâ€  Kevin Stoneâ€ \\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b90527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents([docs[0].page_content])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d850626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f9530f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relavent_doc = vectorstore.similarity_search(\"What is Llama finetuning experiment benchmark ?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60fd7bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(relavent_doc[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c880197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c72e37b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a6a2268e-9a0a-4f17-bbd0-d8f2b12265c9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Kranthi\\\\OneDrive\\\\Desktop\\\\LLMOPS_Krishnaik\\\\Projects\\\\Document_Portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 36, 'page_label': '37'}, page_content='stochastic parrots: Can language models be too big? InProceedings of the 2021 ACM conference on fairness,\\naccountability, and transparency, pages 610â€“623, 2021b.\\n37'),\n",
       " Document(id='24d9ef80-d371-4d43-8572-4cfa4822b8aa', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Kranthi\\\\OneDrive\\\\Desktop\\\\LLMOPS_Krishnaik\\\\Projects\\\\Document_Portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='b1cb9aab-adc4-4ab2-9359-dcb29911165b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Kranthi\\\\OneDrive\\\\Desktop\\\\LLMOPS_Krishnaik\\\\Projects\\\\Document_Portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 10, 'page_label': '11'}, page_content='Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the\\n11'),\n",
       " Document(id='fa5b6793-ab02-4253-901d-95349cbbf696', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Kranthi\\\\OneDrive\\\\Desktop\\\\LLMOPS_Krishnaik\\\\Projects\\\\Document_Portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 45, 'page_label': '46'}, page_content='A.1.1 Acknowledgments\\nThis work was made possible by a large group of contributors. We extend our gratitude to the following\\npeople for their assistance:\\nâ€¢ Our human annotators, whose work we have shown is key to improving tuned model performance,\\nas well as internal leads who organized annotations and quality control: Eric Alamillo, Tamara\\nBest, Debanjali Bose, Adam Kelsey, Meghan Keneally, Rebecca Kogen, Catalina Mejiia, Elisabeth')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetunining benchmark experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36bbdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5063d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b174906",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be9e4c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4731b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25bc8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95b6ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c2c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a2bf780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s see. The user is asking about the Llama 2 fine-tuning benchmark experiments. First, I need to look through the provided context to find relevant information.\\n\\nLooking at the context, there\\'s a section that mentions \"Fine-tuned\" and some numbers associated with different models like Llama 2 7B, 13B, 34B, 70B, and others like ChatGPT, MPT-instruct, Falcon-instruct. The numbers might be performance metrics for these models. \\n\\nThe first part of the context has Llama 2 with various sizes followed by a series of numbers. Then there\\'s a mention of \"Fine-tuned\" under each model size. The second part also has Llama 2 and other models with numbers again. The numbers could represent scores on different benchmarks or evaluation tasks. \\n\\nAdditionally, there\\'s a part about training parameters: final learning rate down to 10% of the peak, weight decay of 0.1, gradient clipping of 1.0. It also mentions training on custom libraries and Meta\\'s clusters. However, this is under the \"Hardware and Software\" section, which might relate more to pretraining than fine-tuning.\\n\\nThe user is specifically asking about fine-tuning benchmarks. The context includes several lines after \"Fine-tuned\" with different models and numbers. But the numbers aren\\'t labeled with what they represent. For example, Llama 2 7B has 0.28, 0.25, etc., but without knowing the benchmarks these correspond to, it\\'s hard to interpret. Similarly, the second set of numbers under \"Fine-tuned\" for Llama 2 and other models might be results on different tasks. However, the context doesn\\'t specify which benchmarks these are, like accuracy on specific datasets or evaluation metrics.\\n\\nSince the answer needs to be based on the context and if there\\'s not enough info, I should say I don\\'t have enough. The problem is that while the context lists numbers for fine-tuned models, it doesn\\'t explain what those numbers mean. Without knowing the benchmarks or the tasks they\\'re measured on, I can\\'t provide a meaningful answer. The user is asking about the experiments done in the fine-tuning benchmarks, but the context lacks the necessary details like benchmark names, evaluation criteria, or comparisons. Therefore, the correct response is to state that there\\'s insufficient information.\\n</think>\\n\\nI do not have enough information about this.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "894782e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s see. The user is asking about the scaling trends for the reward model. I need to check the context provided to find relevant information.\\n\\nLooking through the context, there\\'s a section that mentions studying scaling trends in terms of data and model size. It says that larger models get higher performance with similar data volumes. Also, they talk about different model sizes trained on increasing data each week. The key points are that bigger models perform better with the same data, and there\\'s a figure (Figure 6) showing this. Additionally, it mentions that scaling performance is important, but the context cuts off mid-sentence, so maybe there\\'s more but it\\'s not here.\\n\\nThe user\\'s question is specifically about scaling trends. The context does provide some info: larger models have higher performance for similar data volumes. However, the context seems incomplete, especially since it stops at \"More importantly, the scaling performance...\" without finishing. But what\\'s there does answer part of the question. I should make sure not to include information beyond what\\'s provided. Also, other parts of the context discuss margin-based loss and reward score distributions, but those might not be directly relevant to the scaling trends question. \\n\\nSo the answer should focus on the part about larger models performing better with the same data volume. Even though the context is a bit fragmented, there\\'s enough to state that scaling up the model size leads to better performance. Since the user is asking for scaling trends, which typically involve how performance changes with model size or data size, the context mentions both data and model size scaling. The answer should mention both aspects if possible. However, the exact details on data scaling (like how performance increases with more data) aren\\'t fully elaborated, but the model size part is clear. \\n\\nTherefore, the answer should state that larger models achieve higher performance with similar data volumes, and that scaling model size improves performance. The context also mentions that they studied scaling with increasing data each week, but the results on data scaling aren\\'t detailed here. Since the user is asking for scaling trends, the model size part is covered, and maybe mention data scaling trends if there\\'s info. But the context only says they looked at data and model size, but the results on data scaling aren\\'t specified beyond the model size effect. So stick to what\\'s there.\\n</think>\\n\\nThe context indicates that scaling trends for the reward model show that **larger models achieve higher performance with a similar volume of data** compared to smaller models. Additionally, the study examined scaling in terms of both model size and data volume (e.g., increasing the amount of reward model data collected weekly). However, the context is incomplete and does not provide full details on how performance scales with data size alone. The key takeaway is that model size positively impacts performance when data volume is held constant. \\n\\nSource: *\"larger models obtain higher performance for a similar volume of data\"* (from the context).'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me Scaling trends for the reward model?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_env (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
